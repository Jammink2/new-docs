Quickstart Guide
================

Let's get started with Treasure Data! Treasure Data is a Hadoop-based **Cloud Data Warehousing** service. Forget about servers, storage, and infrastructure while storing billions of records and focusing on data analytics.

Step 1: Sign-Up
---------------

[Sign up](http://treasure-data.com/signup.html) for Treasure Data if you haven't yet.

Step 2: Install Treasure Data Toolbelt
------------------------------------------

Install the [Treasure Data Toolbelt](http://toolbelt.treasure-data.com/) for your development environment. It contains `td` command, a [CLI tool](/categories/command-line) for importing, managing, and querying your data.

* [MacOS Installer](http://crm.treasure-data.com/lead/download_toolbelt?url=http%3A%2F%2Ftoolbelt.treasure-data.com%2Fmac)
* [Windows Installer](http://crm.treasure-data.com/lead/download_toolbelt?url=http%3A%2F%2Ftoolbelt.treasure-data.com%2Fwin)
* Linux ([Redhat/CentOS](http://crm.treasure-data.com/lead/download_toolbelt?url=http%3A%2F%2Fhelp.treasure-data.com%2Fkb%2Finstallation%2Finstalling-td-command-on-redhat-and-centos), [Debian/Ubuntu](http://crm.treasure-data.com/lead/download_toolbelt?url=http%3A%2F%2Fhelp.treasure-data.com%2Fkb%2Finstallation%2Finstalling-td-command-on-debian-and-ubuntu))
* or '[gem install td](http://crm.treasure-data.com/lead/download_toolbelt?url=https%3A%2F%2Frubygems.org%2Fgems%2Ftd)' for those who are familiar with Ruby. `td` works both on 1.8 and 1.9.

Step 3: Authorize
-----------------

After installing the toolbelt, you'll have access to `td` command from your command line. Authorize your account with `td account`. You would be prompted to enter your user name and password. Use the ones from the sign-up process.

    :::term
    $ td account -f
    User name: k@example.com
    Password (typing will be hidden): 
    Authenticated successfully.

Step 4: Import Sample Dataset
-----------------------------

Now you’re ready! Let’s get your feet wet by importing a sample Apache log. You first need to create a database and a table on the cloud via CLI.

    :::term
    $ td db:create testdb
    $ td table:create testdb www_access
    Table 'testdb.www_access' is created.

Let’s generate a sample Apache log and import it to the cloud. `td sample:apache` generates 5,000 lines of Apache log data as JSON. td table:import takes a JSON file and uploads it to the cloud.

    :::term
    $ td sample:apache apache.json
    $ td table:import testdb www_access --json apache.json
    $ tail -n 1 apache.json
    {"host":"200.129.205.208","user":"-","method":"GET","path":"/category/electronics","code":200,"referer":"-","size":62,"agent":"Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/535.11 (KHTML, like Gecko) Chrome/17.0.963.56 Safari/535.11","time":1334035906}

After 20 to 60 seconds, the records will be imported into our database.

    :::term
    $ td tables
    +------------+------------+------+-----------+
    | Database   | Table      | Type | Count     |
    +------------+------------+------+-----------+
    | testdb     | www_access | log  | 5000      |
    +------------+------------+------+-----------+

Step 5: Write Queries
-----------------------

Finally, let’s write a SQL query. A Hadoop-based engine on the cloud executes your query and returns the result to you. The following query calculates the distribution of HTTP status codes.

The above command takes roughly 15 seconds. It’s the overhead of setting up the jobs within the cloud based on Hadoop.

    :::term
    $ td query -w -d testdb \
      "SELECT v['code'], COUNT(1) FROM www_access GROUP BY v['code']"
    queued...
    started at 2012-04-10T23:44:41Z
    2012-04-10 23:43:12,692 Stage-1 map = 0%,  reduce = 0%
    2012-04-10 23:43:18,766 Stage-1 map = 100%,  reduce = 0%
    2012-04-10 23:43:29,925 Stage-1 map = 100%,  reduce = 33%
    2012-04-10 23:43:32,973 Stage-1 map = 100%,  reduce = 100%
    Status     : success
    Result     :
    +------+------+
    | code | cnt  |
    +------+------+
    | 200  | 4981 |
    | 404  | 17   |
    | 500  | 2    |
    +------+------+

Step 6: Logging from Your Application
-------------------------------------

You are ready to import *your real data* to the cloud. See the following tutorials to learn how to import your data from various sources.

  * Languages and Frameworks
    * [Java](java)
    * [Ruby](ruby) or [Rails](rails)
    * [Python](python)
    * [PHP](php)
    * [Perl](perl)
    * [Node.js](nodejs)
    * [Scala](scala)
  * Middleware
    * [Apache Logs](apache)
  * Others
    * [Existing Logs](td-agent-tail)
