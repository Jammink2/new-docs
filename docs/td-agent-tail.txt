# Tailing the Existing Log Files

td-agent can "tail" log files like UNIX's *tail* command and import them into the cloud.

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasure-data.com).
  * Basic knowledge of [td-agent](/articles/td-agent).

## Tailing JSON-based Logs

We are using the [tail input plugin](http://fluentd.org/doc/plugin.html#tail) with the following configuration file. **We assume that each line of the log corresponds to a well-formed JSON (should not span multiple lines).**

NOTE: This feature is supported for td-agent v1.1.5.1 and higher.

    :::term
    <source>
      type tail
      path /path/to/the/file
      tag td.test_db.test_table
      format json
      pos_file /var/log/td-agent/test_db_test_table.pos
    </source>
    
    <match td.*.*>
      type tdlog
      apikey ...
      auto_create_table
      buffer_type file
      buffer_path /var/log/td-agent/buffer/td
    </match>

The example log file is like below.

    :::term
    {"a"=>"b", "c"=>"d"}
    {"a"=>"b", "c"=>"d", "e"=>1}
    {"a"=>"b", "c"=>"d", "e"=>1, "f"=>2.0}
    {"a"=>"b", "c"=>"d"}
    {"a"=>"b", "c"=>"d", "e"=>1}

Every time a new line is appended to the log, td-agent parses the line and buffers it. One can flush the buffer by sending the SIGUSR1 signal.

Now, let's test we have configured everything correctly.

    :::term
    # append new entries
    $ tail -n 3 /path/to/the/file >sample.txt # taking the last three lines of the log...
    $ cat sample.txt >>/path/to/the/file      # and appending them to the file to trigger the tail plugin.
    
    # flush the buffer
    $ kill -USR1 `cat /var/run/td-agent/td-agent.pid`
    
    # confirm the upload
    $ td tables test_db

NOTE: <b>td-agent takes care of log-rotation</b>. By continuously checkpointing the last position of the log, td-agent ensures that each line is read exactly once even if the td-agent process goes down. There is one exception to this "exactly once" guarantee: because the last position of the log is tracked in a file, the exactly once guarantee breaks if this file gets corrupted.

## Tailing Custom-Formatted Logs

If you already have logs with a custom format, you might need to write a custom parser [(Here is how.)](http://fluentd.org/doc/devel.html#custom-parser-for-tail-input-plugin). Once you are done writing the parser, put the file into your */etc/td-agent/plugins/* directory.

We have written up two example parsers, "URL-param style key-value pairs" and "ascii character delimited format". Both of them seem to be fairly common.

    :::term
    # URL-param style key-value pairs
    last_name=smith&first_name=brian&age=22&state=CA
    
    # ASCII character delimited format. In this case, the delimiter is '|'.
    # Usually, there is a separate file that annotates column names
    smith|brian|22|CA 

* [The Custom Parser for URL-Param Style Key-Value Pairs](https://gist.github.com/2565478)
* [The Custom Parser for Ascii-Character Delimited Logs](https://gist.github.com/2565493)

NOTE: Tailing existing logs is by far the easiest way to get started with Treasure Data. We recommend logging everything as JSON. <a href="http://blog.treasure-data.com/post/21881575472/log-everything-as-json-make-your-life-easier">Here's why</a>.

## Filtering Out the Records

If you need to filter logs (ex: filtering out impressions and just keeping clicks), [the exec-filter plugin](http://fluentd.org/doc/plugin.html#exec-filter) comes in handy. This plugin launches another script, which takes STDIN as input and STDOUT as output, and filters logs accordingly.

Here's an example configuration.

    :::term
    <source>
      type tail
      path /path/to/the/file1
      tag filter
      format json
      pos_file /var/log/td-agent/file1.pos
    </source>
    
    <match filter>
      type exec_filter
      command ruby /etc/td-agent/filter.rb
      in_format json  # takes JSON string from STDIN
      out_format json # generate JSON string to STDOUT
      tag_key tag     # The key for tags is "tag".
      time_key time   # The key for timestamps is "time".
    </match>
    
    <match td.*.*>
      type tdlog
      apikey ...
      auto_create_table
      buffer_type file
      buffer_path /var/log/td-agent/buffer/td
    </match>

`/etc/td-agent/filter.rb` is the filter script (shown below). It filters out all the lines where the field "field0" is "certain_value".

    :::ruby
    require 'json'
    while line = STDIN.gets
      # parse
      begin
        h = JSON.parse line
      rescue => e
        next # broken line
      end
      # filter
      next if h["field0"] == "certain_value"
      # emit
      h['tag'] = 'td.test_db.test_table'
      puts h.to_json
      STDOUT.flush
    end

