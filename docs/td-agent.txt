# Continuous Data Import with td-agent

This guide covers how to continuously import the data with `td-agent` daemon.

## Logs Are Streams, Not Files

Logs are usually rotated on an hourly or daily basis based on time or size. This system quickly produces a bunch of huge log files that needs to be batch imported for further analysis. This is an outdated approach. Logs are better thought to be continuously generated *STREAMS* as opposed to files.

NOTE: "Server daemons (such as PostgreSQL or Nginx) and applications (such as a Rails or Django app) sometimes offer a configuration parameter for a path to the programâ€™s logfile. This can lead us to think of logs as files. But a better conceptual model is to treat logs as time-ordered streams..." - <a href="http://adam.heroku.com/past/2011/4/1/logs_are_streams_not_files/">Logs Are Streams, Not Files</a> Adam Wiggins, Heroku co-founder.

You can use the data collection daemon called `td-agent` to import data continuously to Treasure Data. We also support bulk-import, but we recommend importing your data continuously via `td-agent`.

## About td-agent

td-agent is a data collection daemon. It collects logs from various data sources and uploads them to Treasure Data.

<center><img src="/images/td-agent.png" width="90%"></center>

NOTE: `td-agent` is fully open-sourced as <a href="http://fluentd.org/">the fluentd project</a>. `td-agent` is just the package which contains fluentd with some plugins for Treasure Data. You can read up on the difference between `td-agent` and `fluentd` <a href="http://community.fluentd.org/a/what-is-the-difference-with-td-agent-and-fluentd/">here</a>.

## Install td-agent

Please consult the following articles for setup.

<table>
  <tr>
    <th>If you have...</th>
    <th>Please look at...</th>
  </tr>
  <tr>
    <td>Debian / Ubuntu System</td>
    <td><a href="http://treasure-data.tenderapp.com/kb/installing-td-agent-daemon/installing-td-agent-for-debian-and-ubuntu">Installing td-agent for Debian and Ubuntu</a></td>
  </tr>
  <tr>
    <td>Redhat / CentOS System</td>
    <td><a href="http://treasure-data.tenderapp.com/kb/installing-td-agent-daemon/installing-td-agent-for-redhat-and-centos">Installing td-agent for Redhat and CentOS</a></td>
  </tr>
</table>

## Setup td-agent

After installing `td-agent`, you can modify your config file. It's installed at /etc/td-agent/td-agent.conf.

The file includes some sample settings. You should find the following lines in your config.

    :::term
    # HTTP input
    <source>
      type http
      port 8888
    </source>
    
    # Treasure Data output
    <match td.*.*>
      type tdlog
      apikey ...
      auto_create_table
      buffer_type file
      buffer_path /var/log/td-agent/buffer/td
      use_ssl true
    </match>

You need to set the `apikey` option, which is a secret key to authenticate your account. Copy the apikey from ~/.td/td.conf file:

    :::term
    $ grep apikey ~/.td/td.conf
    apikey = 1be3de7d01be3de7d01be3de7d01be3de7d01be3

Now restart the td-agent service.

    :::term
    $ /etc/init.d/td-agent restart

### Confirm Data Upload

You can add logs in JSON using HTTP.

    :::term
    $ curl -X POST -d 'json={"action":"login","user":2}' \
      http://localhost:8888/td.testdb.www_access

`td-agent` continuously uploads logs every minute. By sending SIGUSR1 signal, you can force `td-agent` to flush the buffered logs into the cloud.

    :::term
    $ kill -USR1 `cat /var/run/td-agent/td-agent.pid`

You can confirm that your data was imported successfully by running `td tables`:

    :::term
    $ td tables
    +------------+------------+------+-----------+
    | Database   | Table      | Type | Count     |
    +------------+------------+------+-----------+
    | testdb     | www_access | log  | 1         |
    +------------+------------+------+-----------+

If you run into an issue, your log (/var/log/td-agent.log) is a good place to start your investigation.

## Files Installed by the Packages

Those files are installed by the rpm or debian packages.

<table>
  <tr>
    <th>Resource</th>
    <th>Location</th>
    <th>Notes</th>
  </tr>
  <tr>
    <td>Config Directory</td>
    <td>/etc/td-agent/</td>
    <td></td>
  </tr>
  <tr>
    <td>Config File</td>
    <td>/etc/td-agent/td-agent.conf</td>
    <td>This config will be picked-up by the startup script</td>
  </tr>
  <tr>
    <td>Startup Script</td>
    <td>/etc/init.d/td-agent</td>
    <td></td>
  </tr>
  <tr>
    <td>Log Directory</td>
    <td>/var/log/td-agent/</td>
    <td></td>
  </tr>
  <tr>
    <td>Plugin Directory</td>
    <td>/etc/td-agent/plugin/</td>
    <td>Your custom plugins goes here.</td>
  </tr>
  <tr>
    <td>Ruby Interpreter</td>
    <td>/usr/lib{64}/fluent/ruby/bin/ruby</td>
    <td>Ruby v1.9 is bundled with the package.</td>
  </tr>
  <tr>
    <td>jemalloc</td>
    <td>/usr/lib{64}/fluent/jemalloc/lib/libjemalloc.so</td>
    <td><a href="http://www.canonware.com/jemalloc/">jemalloc</a> is bundled together to avoid memory fragmentation. Loaded by default in the startup script.</td>
  </tr>
</table>

### What's Next?

Next, you need to modify your existing applications to post data to Treasure Data. The articles explain how to do that (with sample code) for various languages, frameworks, and middleware.

  * Languages and Frameworks
    * [Java](java)
    * [Ruby](ruby) or [Rails](rails)
    * [Python](python)
    * [PHP](php)
    * [Perl](perl)
    * [Node.js](nodejs)
    * [Scala](scala)
  * Middleware
    * [Apache Logs](apache)
  * Others
    * [Existing Logs](td-agent-tail)

For high-traffic websites, we recommend using high availability configuration for td-agent.

  * [High-Availability Configurations with td-agent](td-agent-high-availability)

