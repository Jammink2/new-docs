# One-Time Import

This article explains how to import data using the `td table:import` command.

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasure-data.com).

## Three Types of Data Import

You can import data to Treasure Data in three ways. Please choose the option that best suits your needs.

#### 1) Streaming Import

Streaming import is used to import continuously generated data. Please refer to the [td-agent](td-agent) article for further information.

#### 2) Bulk Import

Bulk import is used to import huge amounts of data which could cause a network transfer failure between your system and Treasure Data. Please refer to the [Bulk Import](bulk-import) article for further information. The article explains the additional steps required to enbale multi-part upload, data validation, and transactional commits.

#### 3) One-Time Import

One-time import is used to import small amount of data. Such data is usually stored within one file and rarely causes transfer failrues. The following sections explain how to perform one-time data import.

## The *td table:import* Command

To perform one-time import, please use the `td table:import` command. This command takes the file path and data format as arguments and uploads the data into the cloud. It currently supports the following data formats.

### JSON

In order to perform one-time import with the JSON data format option, the files should be formatted as follows:

* One JSON-map per line. 
* A `time` field is required. This field indicates the time in which the event was generated.

For example, entries could be formatted as follows:

    :::term
    {"action":"login","user":2,"time":"2011-08-02 03:06:32 +0900"}
    {"action":"login","user":4,"time":"2011-08-02 03:06:32 +0900"}
    {"action":"login","user":0,"time":"2011-08-02 03:06:32 +0900"}
    ...

You can import your data into the `test_table` table within your `test_db` database as follows.

    :::term
    $ td table:import test_db test_table \
      --format json \
      --time-key time \
      file1.json file2.json

### Apache Log

The `td table:import` command can also parse Apache Logs stored in the 'combined' format. The command automatically splits the log records into meaningful fields (ex: time, user-agent, etc.) and imports the data into the cloud.

    :::term
    $ td table:import test_db test_table --format apache access_log.txt

### Syslog

The `td table:import` command can also parse logs stored in the 'syslog' format.

    :::term
    $ td table:import test_db test_table --format syslog syslog.txt
