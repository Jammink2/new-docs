# Presto Performance Tuning

## Prerequisites

  * Basic knowledge of Treasure Data.
  * Basic knowledge of [Presto query engine](presto).

## Leveraging Time-based partitioning

All imported data is automatically partitioned into hourly buckets, based on the **'time'** field within each data record. **By specifying the time range to query, you avoid reading unnecessary data and can thus speed up your query significantly.**

### 1) WHERE time <=> Integer

**When the 'time' field within the WHERE clause is specified**, the query parser will automatically detect which partition(s) should be processed. Please note that this auto detection will not work *if you specify the time with `float` instead of `int`.*

    :::sql
    [GOOD]: SELECT field1, field2, field3 FROM tbl WHERE time > 1349393020
    [GOOD]: SELECT field1, field2, field3 FROM tbl WHERE time > 1349393020 + 3600
    [GOOD]: SELECT field1, field2, field3 FROM tbl WHERE time > 1349393020 - 3600
    [BAD]:  SELECT field1, field2, field3 FROM tbl WHERE time > 13493930200 / 10
    [BAD]:  SELECT field1, field2, field3 FROM tbl WHERE time > 1349393020.00
    [BAD]:  SELECT field1, field2, field3 FROM tbl WHERE time BETWEEN 1349392000 AND 1349394000

### 2) <tt>TD_TIME_RANGE</tt>

**An easier way to slice the data is to use [TD_TIME_RANGE UDF](presto-udfs#tdtimerange)**.

    :::sql
    [GOOD]: SELECT ... WHERE TD_TIME_RANGE(time, '2013-01-01 PDT')
    [GOOD]: SELECT ... WHERE TD_TIME_RANGE(time, '2013-01-01', NULL, 'PDT')
    [GOOD]: SELECT ... WHERE TD_TIME_RANGE(time, '2013-01-01',
                                           TD_TIME_ADD('2013-01-01', '1day', 'PDT'))

However, if you use TD_TIME_FORMAT UDF or division in TD_TIME_RANGE, time partition opimization doesn't work. For instance, the following conditions disable optimization.

    :::sql
    [BAD]: SELECT ... WHERE TD_TIME_RANGE(time, TD_TIME_FORMAT(TD_SCHEDULED_TIME(), 'yyyy-MM-dd'))
    [BAD]: SELECT ... WHERE TD_TIME_RANGE(time, TD_TIME_FORMAT(1356998401, 'yyyy-MM-dd'))
    [BAD]: SELECT ... WHERE TD_TIME_RANGE(time, TD_SCHEDULED_TIME() / 86400 * 86400))
    [BAD]: SELECT ... WHERE TD_TIME_RANGE(time, 1356998401 / 86400 * 86400))

## Considering the cardinality within GROUP BY

There's a probability where GROUP BY becomes a little bit faster, by carefully ordering a list of fields within GROUP BY in an order of high cardinality.

    :::sql
    good: SELECT GROUP BY uid, gender
    bad:  SELECT GROUP BY gender, uid

## Use approximate aggregate functions

Presto has a couple of [approximate aggregation functions](http://prestodb.io/docs/current/functions/aggregate.html), which will give you significant performance improvements with some errors. For example by using `approx_distinct()` function, you can get an approximation of `COUNT(DISTINCT x)` with standard error of 2.3%. The example below calculates the approximate unique users of the previous day.

    :::sql
    SELECT
      approx_distinct(user_id)
    FROM
      access
    WHERE
      TD_TIME_RANGE(
        TD_TIME_ADD(TD_SCHEDULED_TIME(), '-1d', 'PDT'),
        TD_SCHEDULED_TIME())

## Aggregate a series of LIKE clauses in one single regexp_like clause

Presto's query optimizer is unable to improve queries where many <tt>LIKE</tt> clauses are used.
As a consequence the query execution can be slower than expected in this case.

To improve the performance, one can substitute a series of <tt>LIKE</tt> clauses chainied in an OR with a single <tt>regexp_like</tt> clause,
which is Presto native.

For example:

    :::sql
    SELECT
      ...
    FROM
      access
    WHERE
      method LIKE '%GET%' OR
      method LIKE '%POST%' OR
      method LIKE '%PUT%' OR
      method LIKE '%DELETE%'

can be optimized by replacing the 4 LIKE clauses with a single <tt>regexp_like</tt> clause:

    :::sql
    SELECT
      ...
    FROM
      access
    WHERE
      regexp_like(method, 'GET|POST|PUT|DELETE')


## Specify large tables first in join clause

Our Presto service uses the distributed hash join algorithm by default, which partitions both left and right table using hash values of join keys. Each partition of the left table is streamed and joined with a partition of the right table stored in a worker node. A limitation is the size of the entire right table must fit your available memory resources.

If you see 'Task exceeded max memory size' error in join queries, it ususally means the right table doesn't fit to the memory. Presto does not perform automatic join-reordering, so please make sure your large table preceeds small tables in a join clause.

## Turning off the distributed hash join

Hash join is known to be slow when there is a skew in your data. If your right table is small enough to fit within one node (usuallly less than 2GB), you can turn the distributed join off by emedding a session property as an SQL comment:

     -- set session distributed_join = 'false'
    SELECT ... FROM large_table l, small_table s WHERE l.id = s.id

