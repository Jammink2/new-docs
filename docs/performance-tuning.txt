# Performance Tuning

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasure-data.com).
  * Basic knowledge of [td-agent](td-agent).
  * Basic knowledge of [the query language](hive).

## Leveraging Time-based Partitioning

All imported data is automatically partitioned into hourly buckets, based on the **'time'** field within each data record. **By specifying the time range to query, you avoid reading unnecessary data and can thus speed up your query significantly.**

### 1) WHERE time <=> Integer

**When the 'time' field within the WHERE clause is specified**, the query parser will automatically detect which partition(s) should be processed. Please note that this auto detection will not work *if you specify the time with `float` instead of `int`.*

    :::sql
    [GOOD]: SELECT field1, field2, field3 FROM tbl WHERE time > 1349393020
    [GOOD]: SELECT field1, field2, field3 FROM tbl WHERE time > 1349393020 + 3600
    [GOOD]: SELECT field1, field2, field3 FROM tbl WHERE time > 1349393020 - 3600
    [BAD]:  SELECT field1, field2, field3 FROM tbl WHERE time > 1349393020.00

### 2) TD_TIME_RANGE

**An easier way to slice the data is to use [TD_TIME_RANGE UDF](udfs#tdtimerange)**.

    :::sql
    SELECT ... WHERE TD_TIME_RANGE(time, "2013-01-01 PDT")
    SELECT ... WHERE TD_TIME_RANGE(time, "2013-01-01", NULL, "PDT")
    SELECT ... WHERE TD_TIME_RANGE(time, "2013-01-01",
                                   TD_TIME_ADD("2013-01-01", "1day", "PDT"))

## Set Custom Schema

As explained in [the "Schema Management" article](schema), all tables have two fields: 'v' and 'time'. In addition to these, you can set [custom schema](schema#setting-custom-schema) on the tables.

    :::term
    $ td schema:set testdb www_access action:string user:int
    $ td query -w -d testdb "SELECT user, COUNT(1) AS cnt
         FROM www_access
         WHERE action='login'
         GROUP BY user ORDER BY cnt DESC"

After setting the schema, **queries issued with named columns instead of 'v'** will use the schema information to achieve a more optimized execution path. In particular, GROUP BY performance will improve significantly.
