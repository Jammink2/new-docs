# High Availability td-agent Configuration

This guide covers the advanced configurations of 'td-agent', especially for the environment which requires high availability.

## Prerequisites

  * Basic knowledge of Treasure Data, including the latest installed version of the toolbelt.
  * Basic td-agent setup knowledge

## Message Delivery Semantics

td-agent is designed for event-log delivery system. there are multiple possible delivery guarantees that could be provided:

* *At most once*: Messages are immediately transfered and never retried if it succeeded. So they can't be given out twice, but many failure scenarios may lead to losing messages.
* *At least once*: Each message will be delivered at least once, but in failure cases may be delivered twice.
* *Exactly once*: This is what people actually want, each message is delivered once and only once.

If you say you "can't lose a single event, and they must be transferred *exactly at once*," understand that that means you'll eventually need to stop ingesting events if you run out of the write capacity. The best way to deal with this, is to use synchronous logging and return the proper error responses if you can't accept the event.

By having the proper write capacity and collector failover chains, the message lost will be drastically reduced. Note that you'll get poor performance in this case.

That's why *td-agent takes 'At most once' guarantee*. it's designed for collecting massive amount of data, without impacting the application performance. To achieve that, the logging is done in an asynchronous manner. The performance is much better but the losses of the message delivery could happen.

That said, most loss scenarios are preventable. The following section describes how to setup td-agent topology for HA.

## Network Topology

For high availability, we assume '*log forwarders*' and '*log aggregators*' are located in your network.

<img src="/images/td-agent_ha.png" width="600px">

Usually '*log forwarders*' are installed on every nodes, and receive local events. Once it receives the event, they forward it to the 'log aggregators' via network.

'*log aggregators*' are the daemons, which continuously receives the events from forwarders. They buffer the events, and periodically upload into the cloud.

td-agent can acts *both* as log forwarders and log aggreagators. They can be swiched via configuration. Next section describes how to setup them. We assume active log aggregator has ip '192.168.0.1' and backup has ip '192.168.0.2'.

## Log Forwarder Configuration

At the log forwarder, please add those lines to your /etc/td-agent/td-agent.conf. This will transfer logs to the log aggregators.

    :::term
    # TCP input
    <source>
      type forward
      port 24224
    </source>
    
    # HTTP input
    <source>
      type http
      port 8888
    </source>

    # Log Forwarding
    <match td.*.*>
      type forward

      <server>
        host 192.168.0.1
        port 24224
      </server>
    
      # use secondary host
      <server>
        host 192.168.0.2
        port 24224
        standby
      </server>
    
      # use file buffer to buffer events on disks.
      buffer_type file
      buffer_path /var/log/td-agent/buffer/forward
    
      # use longer flush_interval to reduce CPU usage.
      # this is trade-off of latency.
      flush_interval 60s
    </match>

When the active aggregator (192.168.0.1) dies, the logs are started sending to the backup/secondary aggregator (192.168.0.2). If both servers die, the logs are buffered on the disks at the forwarder node.

## Log Aggregator Configuration

At the log aggregator, please set those lines to /etc/td-agent/td-agent.conf. The log transfer is done via TCP input source.

    :::term
    # TCP input
    <source>
      type forward
      port 24224
    </source>
    
    # Treasure Data output
    <match td.*.*>
      type tdlog
      apikey YOUR_API_KEY_HERE
      auto_create_table
      buffer_type file
      buffer_path /var/log/td-agent/buffer/td
    </match>

The received logs are buffered, and periodically uploaded into the cloud. If the upload fails, the logs are stored on the disks, until retransmission succeeds.

If you want to write logs to file in addition to TD, please use 'copy' output. This is an example configuration of writing logs into TD, file, and MongoDB.

    :::term
    <match td.*.*>
      type copy
      <store>
        type tdlog
        apikey YOUR_API_KEY_HERE
        auto_create_table
        buffer_type file
        buffer_path /var/log/td-agent/buffer/td
      </store>
      <store>
        type file
        path /var/log/td-agent/myapp.%Y-%m-%d-%H.log
        localtime
      </store>
      <store>
        type mongo_replset
        database db
        collection logs
        nodes host0:27017,host1:27018,host2:27019
      </store>
    </match>

## Failure Case Scenarios

### Forwarder Failure

When forwarder receives an event from applications, the events are first written into disk buffer (specified by buffer_path). And after every flush_interval, the buffered data is forwarded to aggregators.

Therefore, if td-agent process dies, the buffered data is properly transferred into aggregators after the restart. If the network between forwarders and aggregators are broken, the data transfer is automatically retried. However, here're some message lost scenarios:

* The process dies, just after receiving the events, but before writing into the buffer.
* The forwarder disk is broken, and file buffer is lost.

### Aggregator Failure

When aggregator receives an event from forwarders, the events are first written into disk buffer (specified by buffer_path). And after every flush_interval, the buffered data is uploaded into the cloud.

Therefore, if td-agent process dies, the buffered data is properly transferred into aggregators after the restart. If the network between aggregators and cloud are broken, the data transfer is automatically retried.

However, here're some message lost scenarios:

* The process dies, just after receiving the events, but before writing into the buffer.
* The forwarder disk is broken, and file buffer is lost.

## What's Next?

Now you've seen the HA configuration for td-agent. For further information, please refer those documents.

* [Fluentd documentation](http://fluent.github.com/doc/) (td-agent is open-sourced as 'Fluentd')
* [Treasure Data Suppport](http://treasure-data.tenderapp.com/discussions)

td-agent is maintained by Treasure Data, Inc. and released continuously. The changelog is available here.

* [ChangeLog of td-agent](http://help.treasure-data.com/kb/installing-td-agent-daemon/changelog-of-td-agent)
