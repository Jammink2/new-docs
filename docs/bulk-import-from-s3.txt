# Bulk Import from Amazon S3

This article explains how to import data directly from Amazon S3 to Treasure Data.

<br/>
## Install Bulk Loader

First, install the [toolbelt](command-line), which includes bulk loader program, on your computer.

#### Downloads
- [Toolbelt Installer for Windows](http://toolbelt.treasuredata.com/win)
- [Toolbelt Installer for Mac OS X](http://toolbelt.treasuredata.com/mac)
- [Toolbelt Installer for Linux](http://toolbelt.treasuredata.com/linux)

After the installation, the `td` command will be installed on your computer. Open up the terminal and type `td` to execute the command. Also, make sure you have `java` as well. Run `td import:jar_update` to download the up-to-date version of our bulk loader:

    :::terminal
    $ td
    usage: td [options] COMMAND [args]
    $ java
    Usage: java [-options] class [args...]
    $ td import:jar_update
    Installed td-import.jar 0.x.xx into /path/to/.td/java

## Importing data from Amazon S3

The bulk loader can read data from files stored in Amazon S3 in all three supported file formats:

* CSV (default)
* JSON
* TSV

Suppose you have a file called <tt>data.csv</tt> on Amazon S3 with these contents:

    :::terminal
    "host","log_name","date_time","method","url","res_code","bytes","referer","user_agent"
    "64.242.88.10","-","2004-03-07 16:05:49","GET","/twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables",401,12846,"",""
    "64.242.88.10","-","2004-03-07 16:06:51","GET","/twiki/bin/rdiff/TWiki/NewUserTemplate?rev1=1.3&rev2=1.2",200,4523,"",""
    "64.242.88.10","-","2004-03-07 16:10:02","GET","/mailman/listinfo/hsdivision",200,6291,"",""
    "64.242.88.10","-","2004-03-07 16:11:58","GET","/twiki/bin/view/TWiki/WikiSyntax",200,7352,"",""

Execute the following commands to upload the CSV file:

    $ td db:create my_db
    $ td table:create my_db my_tbl
    $ td import:auto \
      --format csv --column-header \
      --time-column date_time \
      --time-format "%Y-%m-%d %H:%M:%S" \
      --auto-create my_db.my_tbl \
      "s3://<s3_access_key>:<s3_secret_key>@/my_bucket/path/to/data.csv"

where the location of the file is expressed as an S3 path with the AWS public and private access keys embedded in it.

NOTE: Because `td import:auto` executes MapReduce jobs to check the invalid rows, it'll take at least <b>1-2 minutes</b>.

In the above command, we assumed that:

* The CSV files are located on Amazon S3, within a bucket called `my_bucket` under this path/key `/path/to/`.
* The first line in the file indicates the column names, hence we specify the <tt>\-\-column-header</tt> option.
  If the file does not have the column names in the first row, you will have to specify the column names with the
  <tt>\-\-columns</tt> option (and optionally the column types with <tt>\-\-column-types</tt> option), or use the
  <tt>\-\-column-types</tt> for each column in the file.
* The time field is called "date_time" and it's specified with the <tt>\-\-time-column</tt> option
* The time format is <tt>%Y-%m-%d %H:%M:%S</tt> and it's specified with the <tt>\-\-time-format</tt> option

<br/>
### Wildcards

The source files to be imported by the bulk loader can be specified as full Amazon S3 paths or using wildcards.
Here are some examples:

* <tt>s3://my_bucket/path/to/data*</tt><br/>
  All files under <tt>my_bucket/path/to/</tt> with prefix <tt>data</tt>;
* <tt>s3://my_bucket/path/to/data*.csv</tt><br/>
  All files under <tt>my_bucket/path/to/</tt> with prefix <tt>data</tt> and extension <tt>.csv</tt>;
* <tt>s3://my_bucket/path/to/*.csv</tt><br/>
  All files under <tt>my_bucket/path/to/</tt> with extension <tt>.csv</tt>;
* <tt>s3://my_bucket/path/to/*</tt><br/>
  All files under <tt>my_bucket/path/to/</tt>;
* <tt>s3://my_bucket/path/to/\*/*.csv</tt><br/>
  All files in the direct subfolders of <tt>my_bucket/path/</tt> with extension <tt>.csv</tt>;
* <tt>s3://my_bucket/\*\*/*.csv</tt><br/>
  All files in all subfolders of <tt>my_bucket/path/</tt> with extension <tt>.csv</tt>;

For further details, check the following pages:

- [Bulk Import Internals](bulk-import-internal).
- [Bulk Import Tips and Tricks](bulk-import-tips-and-tricks)
