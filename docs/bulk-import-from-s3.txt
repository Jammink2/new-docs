# Bulk Import from Amazon S3

This article explains how to import data from Amazon S3 to Treasure Data.

## Install Bulk Loader

First, please install the [toolbelt](command-line), which includes bulk loader program, on your computer.

#### Downloads
- [Toolbelt Installer for Windows](http://toolbelt.treasuredata.com/win)
- [Toolbelt Installer for Mac OS X](http://toolbelt.treasuredata.com/mac)
- [Toolbelt Installer for Linux](http://toolbelt.treasuredata.com/linux)

After the installation, `td` command will be installed on your computer. Please open up the terminal, and type `td` to execute the command. Also, please make sure you have `java` as well. Please execute `td import:jar_update` to download the up-to-date version of our bulk loader:

    :::term
    $ td
    usage: td [options] COMMAND [args]
    $ java
    Usage: java [-options] class [args...]
    $ td import:jar_update
    Installed td-import.jar 0.x.xx into /path/to/.td/java

## Importing data from Amazon S3

Suppose you have a file called <tt>data.csv</tt> on Amazon S3 and its content is like this:

    :::term
    "host","log_name","date_time","method","url","res_code","bytes","referer","user_agent"
    "64.242.88.10","-","2004-03-07 16:05:49","GET","/twiki/bin/edit/Main/Double_bounce_sender?topicparent=Main.ConfigurationVariables",401,12846,"",""
    "64.242.88.10","-","2004-03-07 16:06:51","GET","/twiki/bin/rdiff/TWiki/NewUserTemplate?rev1=1.3&rev2=1.2",200,4523,"",""
    "64.242.88.10","-","2004-03-07 16:10:02","GET","/mailman/listinfo/hsdivision",200,6291,"",""
    "64.242.88.10","-","2004-03-07 16:11:58","GET","/twiki/bin/view/TWiki/WikiSyntax",200,7352,"",""

Next, please execute the following commands to upload the CSV file:

    $ td db:create my_db
    $ td table:create my_db my_tbl
    $ td import:auto \
      --format csv --column-header \
      --time-column date_time \
      --time-format "%Y-%m-%d %H:%M:%S" \
      --auto-create my_db.my_tbl \
      "s3://<s3_access_key>:<s3_secret_key>@/my_bucket/path/to/*.csv"

NOTE: Because `td import:auto` executes the MapReduce jobs to check the invalid rows, it'll take at least <b>1-2 minutes</b>.

In the above command, we assumed that:

* The CSV files are located on Amazon S3, within a bucket called `my_bucket` under this path/key `/path/to/`.
* The first line in the file indicates the column names, hence we specify the <tt>\-\-column-header</tt> option.
  If the file does not have the column names in the first row, you will have to specify the column names with the
  <tt>\-\-columns</tt> option (and optionally the column types with <tt>\-\-column-types</tt> option), or use the
  <tt>\-\-column-types</tt> for each column in the file.
* The time field is called "date_time" and it's specified with the <tt>\-\-time-column</tt> option
* The time format is <tt>%Y-%m-%d %H:%M:%S</tt> and it's specified with the <tt>\-\-time-format</tt> option

For further details, check the following pages:

- [Bulk Import Internals](bulk-import-internal).
- [Bulk Import Tips and Tricks](bulk-import-tips-and-tricks)
