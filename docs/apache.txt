# Analyzing Apache Logs on the Cloud

[Treasure Data](http://treasure-data.com/) enables you to analyze apache logs on the cloud. You can forget about server, and storages.

This article explains how to import Apache logs with `td-agent`. The basic idea is that td-agent (1) continuously accepts new Apache logs, (2) parses the logs, and (3) periodically uploads the logs into the cloud. td-agent’s `tail` source type is used.

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasure-data.com).
  * Basic knowledge of td-agent.
  * Ruby 1.8 or higher (for local testing).

## Architecture

<center><img src="/images/apache_log_with_td-agent.png" width="90%" /></center>

`td-agent` is a versatile daemon program that can process various kinds of streamed log data. `td-agent` is developed and maintained by [Treasure Data, Inc.](http://www.treasure-data.com).

When a user accesses an Apache web server, the web server appends a corresponding entry to the access log (typically located at */var/log/apache2/access_log*). `td-agent` continuously "tails" the access log, identifying the newly created entries.

`td-agent` automatically parses the new log entries to create meaningful data fields (ex: IP address, URL path, etc.). The data is compressed and uploaded to the cloud periodically.

## Installing td-agent

You must first set up `td-agent` on your application servers. Please refer to the following articles. For Linux systems, we provide deb/rpm packages.

<table>
  <tr>
    <th>If you have...</th>
    <th>Please look at...</th>
  </tr>
  <tr>
    <td>Debian / Ubuntu Systems</td>
    <td><a href="http://treasure-data.tenderapp.com/kb/installing-td-agent-daemon/installing-td-agent-for-debian-and-ubuntu">Installing td-agent for Debian and Ubuntu</a></td>
  </tr>
  <tr>
    <td>Redhat / CentOS Systems</td>
    <td><a href="http://treasure-data.tenderapp.com/kb/installing-td-agent-daemon/installing-td-agent-for-redhat-and-centos">Installing td-agent for Redhat and CentOS</a></td>
  </tr>
</table>

NOTE: td-agent is fully open-sourced under the <a href="http://fluentd.org/">fluentd project</a>. td-agent extends fluentd with custom plugins for Treasure Data.

## Modifying /etc/td-agent/td-agent.conf

In order to import Apache logs with `td-agent`, you need to add the following lines to your `td-agent.conf` file. Note that you need to specify the database and table names using the `tag` parameter.

    :::term
    # tail apache access_log
    <source>
      type tail
      path /var/log/apache2/access_log
      format apache
      tag td.YOUR_DATABASE_NAME.YOUR_TABLE_NAME
    </source>

NOTE: On Redhat based systems, you might need to change the directory permission by: `chmod g+rx /var/log/httpd`.

For example, if you set ‘td.testdb.apache_logs’ as the tag, the logs are sent to the `apache_logs` table within the `testdb` database.
    
Next, you specify your authentication key by setting the `apikey` option. You can view your api key with the `td apikey:show` command.

NOTE: You must first authenticate your account using the `td account` command.

    :::term
    $ td apikey:show
    3b7118fd3ad7e35bbd3c0e4f607ec7263aa93c30

Next, set the `apikey` option in your `td-agent.conf` file. 

Note: *YOUR_API_KEY* should be your actual API key string.

    :::term
    # Treasure Data Input and Output
    <match td.*.*>
      type tdlog
      apikey YOUR_API_KEY
      auto_create_table
      buffer_type file
      buffer_path /var/log/td-agent/buffer/td
      use_ssl true
    </match>

Now, restart `td-agent` to reflect the above changes.

    :::term
    $ sudo /etc/init.d/td-agent restart

`td-agent` now tails Apache logs, parses the new log entries, and uploads the parsed data into the cloud every 5 minutes.

## Confirming Import

Sending a SIGUSR1 signal flushes td-agent's buffer and the data gets uploaded to the cloud immediately (as opposed to the usual 5 minute intervals).

    :::term
    $ curl http://127.0.0.1/
    $ kill -USR1 `cat /var/run/td-agent/td-agent.pid`

You can confirm that data is getting uploaded successfully with the `td tables` command, which shows you how many rows have been saved on the cloud (the "Cloud" column).

    :::term
    $ td tables
    +------------+------------+------+-----------+
    | Database   | Table      | Type | Count     |
    +------------+------------+------+-----------+
    | test_db    | test_table | log  | 21321     |
    +------------+------------+------+-----------+

## Sample Queries

Those are the sample queries to calculate the interesting results.

### Top User Agents

    :::term
    $ td query -w -d testdb \
      "SELECT v['agent'] AS agent, COUNT(1) AS cnt \
       FROM www_access \
       GROUP BY v['agent'] ORDER BY cnt DESC LIMIT 3"
    ...
    Result:
    +---------------------------------------------------------------------------+-----+
    | agent                                                                     | cnt |
    +---------------------------------------------------------------------------+-----+
    | Mozilla/5.0 (Windows NT 6.0; rv:10.0.1) Gecko/20100101 Firefox/10.0.1     | 630 |
    | Mozilla/5.0 (compatible; MSIE 9.0; Windows NT 6.1; WOW64; Trident/5.0)    | 497 |
    | Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)  | 341 |
    +---------------------------------------------------------------------------+-----+

### Top Paths

    :::term
    $ td query -w -d testdb \
      "SELECT v['path'] AS path, COUNT(1) AS cnt \
       FROM www_access \
       GROUP BY v['path'] ORDER BY cnt DESC LIMIT 3"
    ...
    Result:
    +-----------------------+-----+
    | path                  | cnt |
    +-----------------------+-----+
    | /category/electronics | 639 |
    | /category/software    | 416 |
    | /category/books       | 285 |
    +-----------------------+-----+

### Top Referers within a Specific Day

    :::term
    $ td query -w -d testdb \
      "SELECT v['referer'] AS referer, COUNT(1) AS cnt \
       FROM www_access6 \
       WHERE \
         unix_timestamp('2012-05-26', 'yyyy-MM-dd') <= time
         AND time < unix_timestamp('2012-05-27', 'yyyy-MM-dd')
       GROUP BY v['referer'] ORDER BY cnt DESC LIMIT 3"
    ...
    Result:
    +-----------------------+------+
    | referer               | cnt  |
    +-----------------------+------+
    | -                     | 1939 |
    | /category/electronics | 327  |
    | /category/software    | 235  |
    +-----------------------+------+

### Access Count by Day

    :::term
    $ td query -w -d testdb \
      "SELECT to_date(from_unixtime(time)) AS day, COUNT(1) AS cnt \
       FROM www_access \
       GROUP BY to_date(from_unixtime(time))"
    ....
    Result:
    +------------+------+
    | day        | cnt  |
    +------------+------+
    | 2012-05-26 | 5000 |
    | 2012-05-27 | 6000 |
    +------------+------+

### Status Code Distribution by Day

    :::term
    $ td query -w -d testdb \
      "SELECT to_date(from_unixtime(time)) AS day, v['code'] AS code, COUNT(1) AS cnt \
       FROM www_access \
       GROUP BY to_date(from_unixtime(time)), v['code']"
    ....
    Result:
    +------------+------+------+
    | day        | code | cnt  |
    +------------+------+------+
    | 2012-05-26 | 200  | 4981 |
    | 2012-05-26 | 404  | 17   |
    | 2012-05-26 | 500  | 2    |
    | 2012-05-27 | 200  | 4481 |
    | 2012-05-27 | 404  | 517  |
    | 2012-05-27 | 500  | 2    |
    +------------+------+------+

### Min/Max/Avg Size by Day

    :::term
    $ td query -w -d testdb \
      "SELECT to_date(from_unixtime(time)) AS day, \
         max(cast(v['size'] AS INT)) AS max, \
         min(cast(v['size'] AS INT)) AS min, \
         avg(cast(v['size'] AS INT)) AS avg  \
       FROM www_access \
       GROUP BY to_date(from_unixtime(time))"
    ...
    Result:
    +------------+-----+-----+---------+
    | day        | max | min | avg     |
    +------------+-----+-----+---------+
    | 2012-05-26 | 139 | 40  | 89.0056 |
    | 2012-05-27 | 100 | 30  | 95.0056 |
    +------------+-----+-----+---------+
