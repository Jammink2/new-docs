# Presto Query

[Treasure Data](http://treasuredata.com/) is a Hadoop-based Big Data analytics platform. Treasure Data supports Presto as a low-latency query engine. This feature is currently in beta.

## What is Presto?

[Presto](https://github.com/facebook/presto) is an open-source parallel SQL execution engine. Unlike Hive, Presto doesn't use the map reduce framework for its execution. Instead, Presto directly accesses the data through a specialized distributed query engine that is very similar to those found in commercial parallel RDBMSs.

<center><img src="https://scontent-a-sjc.xx.fbcdn.net/hphotos-prn2/t1.0-9/s720x720/1467246_10151935581722200_1107575290_n.png" width="80%" /></center><br />

Treasure Data has customized Presto to talk directly with our distributed columnar storage layer. As a result, the end user experience is nearly identical to querying Hive.

## Does Presto Replace Hive?

**No**. Hive is designed for batch processing, while Presto is designed for short interactive queries useful for data exploration.

Presto currently has limited fault tolerance capabilities when querying. If a process fails while processing, the whole query must be re-run. On the other hand, it executes queries 10-30x faster than Hive. Thus, even if there is a process failure and a query must be restarted, the total runtime will often still beat Hive's significantly. 

Another caveat is that Presto has an in-memory only architecture. So if there is a particularly large data set which exceeds the total memory capacity available to Presto, query execution will fail.

Even with Presto as part of our ecosystem, MapReduce and Hive will continue to have many viable use cases (for example: long-running data transformation workloads).

## How to Use Presto?

Using the [CLI](/articles/command-line), specify `-T presto` in the `td query` command. A v0.10.99 or newer client is required.

    :::term
    $ td query -w -T presto -d testdb \
      "SELECT code, COUNT(1) FROM www_access GROUP BY code"

Alternatively, from the [web console](https://console.treasuredata.com), on the New Query page select "Presto" as the query type.

<center><img src="/images/console-query-presto.png" width="75%" /></center><br />

For REST API, the endpoint is `/v3/job/issue/presto/:database`.

## Presto SQL Language Reference
  * [Presto Function and Operators](http://prestodb.io/docs/current/functions.html)
  * [Presto SQL Syntax](http://prestodb.io/docs/current/sql/select.html)

## Known Limitations

We're currently in the beta phase of Presto development. Naturally, there are some limitations compared to Hive at this point. We welcome your feedback and will make improvements based on these.

### Table Schemas Are Required

When querying via Presto, you must set a [Schema](schema) for the table(s); the `v` syntax doesn't work since Presto doesn't support the `map` type yet.

### Lack of Complex Types

Currently, the only available types in Presto are `boolean`, `bigint`, `double` and `varchar`. Complex types such as `map`, `array`, and `struct` are currently not available.

### Limited Number of UDFs

A subset of TD UDFs are available in Presto. The following is a list of supported UDFs:

* [TD_TIME_RANGE](http://docs.treasuredata.com/articles/udfs#tdtimerange)
* [TD_TIME_ADD](http://docs.treasuredata.com/articles/udfs#tdtimeadd)
* [TD_TIME_FORMAT](http://docs.treasuredata.com/articles/udfs#tdtimeformat)
* [TD_TIME_PARSE](http://docs.treasuredata.com/articles/udfs#tdtimeparse)

### JOIN Order

Presto does not currently support cost-based JOIN optimizations, meaning JOINs are not automatically reordered based on table size. Please make sure that smaller tables are on the right hand size of JOIN, and they must fit in memory. Otherwise out of memory exceptions will cause the query to fail.

    :::sql
    SELECT
      ...
    FROM
      large_table
    JOIN
      small_table


## See Also
* [Presto Documentation](http://prestodb.io/docs/current/)
* [Presto: Interacting with petabytes of data at Facebook](https://www.facebook.com/notes/facebook-engineering/presto-interacting-with-petabytes-of-data-at-facebook/10151786197628920)
