# Bulk Import

This article explains how to bulk-import data with the `td bulk_import` command.

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasure-data.com).

## Three Categories of Import

#### 1) Streaming Import

Importing continuously generated data. In this case, the [td-agent](td-agent) article is more appropriate.

#### 2) Bulk Import

Importing huge amounts of data, which could cause a network transfer failure between your side and Treasure Data. This article covers this case.

#### 3) One-Time Import

If your data size is small (For example, the data fits in a single file), you can do [One-Time-Import](one-time-import) instead. This is easier since your data does not have to be the MessagePack format.

## Why Bulk Import?

Because Treasure Data is a cloud service, the data needs to be transferred via an Internet network connection. This can get tricky once the data size gets big (> 100MB). Consider a couple of cases:

* If the network becomes unstable, the import could fail halfway through. There's no easy way to pick up from where you left off, and you need to restart the upload from the beggining.
* Your company sometimes limits the bandwidth for transferring huge data with a single stream. Also, the limitations of the TCP/IP protocol make it difficult for applications to saturate a network connection.

To overcome these problems, we implemented the bulk import feature. You can now break your larger data sets into smaller chunks, and upload them *in parallel*. If the upload of a chunk fails, you can restart the upload just for that chunk. You'll be able to impove your overall upload speed by taking advantage of parallelism.

## Bulk Import Steps

### 1) Create a Session

At first you need to create a `session`, which is a single instance containing multiple parts. One session is associated with one table.

    :::bash
    $ td bulk_import:create bulk_2012 mydb mytable

### 2) Convert Your Data into Parts Files

Assume you have CSV files within ./logs_201208/ directory. At first, you need convert those files, into MessagePack.gz format which is our internal format. To do that, please perform the command below.

NOTE: Splitting the data into separate directory by month is recommended, but you can also split it by date if you have more than 1GB/day of data post-compression.

    :::bash
    $ td bulk_import:prepare_parts ./logs_201208/*.csv \
        --format csv \
        --columns time,uid,price,count --time-column 'time' \
        -o ./parts_201208/

If you have a column name at the first line, please use --column-header option.

    :::bash
    $ td bulk_import:prepare_parts ./logs_201208/*.csv \
        --format csv \
        --column-header \
        --time-column 'time' \
        -o ./parts_201208/

We supports csv, tsv, and json for --format option. Here're the examples.

    :::bash
    # TSV
    $ td bulk_import:prepare_parts ./logs_201208/*.tsv \
        --format tsv \
        --column-header \
        --time-column 'time' \
        -o ./parts_201208/
    
    # JSON
    $ td bulk_import:prepare_parts ./logs_201208/*.json \
        --format json \
        --time-column 'time' \
        -o ./parts_201208/

### 2) Upload a Part to the Session

Then, you can upload the `parts` directory into the session.

    :::bash
    $ td bulk_import:upload_parts bulk_2012 ./parts_201208/*

If the upload failed in halfway through, you can retry any time. No data duplication occurs.

    $ td bulk_import:upload_parts bulk_2012 ./parts_201208/*

You can import multiple parts within a single session.

    $ td bulk_import:upload_parts bulk_2012 ./parts_201207/*
    $ td bulk_import:upload_parts bulk_2012 ./parts_201206/*
    $ td bulk_import:upload_parts bulk_2012 ./parts_201205/*
    ...

Now, let's list the uploaded parts by `bulk_import:show`.

    :::bash
    $ td bulk_import:show bulk_2012

### 3) Perform

`bulk_import:perform` does a format conversion on our cloud. Your data is converted into a more efficient format (This is why it takes a bit of time!). The conversion is done in parallel by executing MapReduce jobs.

    :::bash
    $ td bulk_import:freeze bulk_2012
    $ td bulk_import:perform bulk_2012 -w
    $ td bulk_import:list
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+
    | Name | Table     | Status | Frozen | JobID | Valid Parts | Error Parts | Valid Records | Error Records |
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+
    | t01  | sfdb.bi02 | Ready  |        | 70220 | 2           | 1           | 100           | 10            |
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+

### 4) Check Error Records

After `perform`, some records could be invalid. You can list these invalid records by `bulk_import:error_records`.

    :::bash
    $ td bulk_import:error_records bulk_2012

### 5) Commit

If the uploaded data is correct, `bulk_import:commit` command will transactionally import the data into the table.

    :::bash
    $ td bulk_import:commit bulk_2012
    $ td bulk_import:delete bulk_2012
