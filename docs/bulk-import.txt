# Bulk Import

This article explains how to bulk-import data with the `td bulk_import` command.

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasure-data.com).

## Three Categories of Import

#### 1) Streaming Import

Importing continuously generated data. In this case, the [td-agent](td-agent) article is more appropriate.

#### 2) Bulk Import

Importing huge amounts of data, which could cause a network transfer failure between your side and Treasure Data. This article covers this case.

#### 3) One-Time Import

If your data size is small (For example, the data fits in a single file), you can do [One-Time-Import](one-time-import) instead. This is easier since your data does not have to be the MessagePack format.

## Why Bulk Import?

Because Treasure Data is a cloud service, the data needs to be transferred via an Internet network connection. This can get tricky once the data size gets big (> 100MB). Consider a couple of cases:

* If the network becomes unstable, the import could fail halfway through. There's no easy way to pick up from where you left off, and you need to restart the upload from the beggining.
* Your company sometimes limits the bandwidth for transferring huge data with a single stream. Also, the limitations of the TCP/IP protocol make it difficult for applications to saturate a network connection.

To overcome these problems, we implemented the bulk import feature. You can now break your larger data sets into smaller chunks, and upload them *in parallel*. If the upload of a chunk fails, you can restart the upload just for that chunk. You'll be able to impove your overall upload speed by taking advantage of parallelism.

## Bulk Import Steps

### 0) Split Your Data into MessagePack.gz Files

At first, you need to split your data into [MessagePack](http://msgpack.org/).gz format. Splitting the data by month is recommended, but you can also split it by date if you have more than 1GB/day of data post-compression.

### 1) Create a Session

At first you need to create a `session`, which is a single instance containing multiple parts. One session is associated with one table.

    :::bash
    $ td bulk_import:create bulk_201203 mydb mytable

### 2) Upload a Part to the Session

Then, you can upload the `parts` into the session. Those uploads can be done in parallel.

    :::bash
    $ td bulk_import:upload_part bulk_201203 01d 01d.msgpack.gz
    $ td bulk_import:upload_part bulk_201203 02d 02d.msgpack.gz
    $ td bulk_import:upload_part bulk_201203 03d 03d.msgpack.gz

If the upload failed in halfway through, you can retry any time.

    $ td bulk_import:upload_part bulk_201203 01d 01d.msgpack.gz

Now, let's list the uploaded parts by `bulk_import:show`.

    :::bash
    $ td bulk_import:show bulk_201203

### 3) Perform

`bulk_import:perform` does a format conversion on our cloud. Your data is converted into a more efficient format (This is why it takes a bit of time!). The conversion is done in parallel by executing MapReduce jobs.

    :::bash
    $ td bulk_import:freeze bulk_201203
    $ td bulk_import:perform bulk_201203 -w
    $ td bulk_import:list
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+
    | Name | Table     | Status | Frozen | JobID | Valid Parts | Error Parts | Valid Records | Error Records |
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+
    | t01  | sfdb.bi02 | Ready  |        | 70220 | 2           | 1           | 100           | 10            |
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+

### 3) Check Error Records

After `perform`, some records could be invalid. You can list these invalid records by `bulk_import:error_records`.

    :::bash
    $ td bulk_import:error_records bulk_201203

### 4) Commit

If the uploaded data is correct, `bulk_import:commit` command will transactionally import the data into the table.

    :::bash
    $ td bulk_import:commit bulk_201203
    $ td bulk_import:delete bulk_201203
