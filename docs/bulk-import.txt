# Bulk Import Overview

This article explains how to bulk-import data using the `td import` command for **version 0.10.84 and above**.

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasuredata.com).
  * Java runtime (6 and above).

## Why Bulk Import?

<center><img src="/images/bulk-import.png" width="100%" /></center><br /><br />

Because Treasure Data is a cloud service, the data needs to be transferred via an Internet network connection. This can get tricky once the data size gets big (> 100MB). Consider a couple of cases:

* If the network becomes unstable, the import could fail halfway through the data transfer. There's no easy way to pick up from where you left off, and you will need to restart the upload from scratch.
* Your company sometimes has bandwidth limits for transferring huge data with a single stream. Also, the limitations of the TCP/IP protocol make it difficult for applications to saturate a network connection.

We designed our bulk import feature to overcome these problems. You can now break your larger data sets into smaller chunks and upload them *in parallel*. If the upload of a particular chunk fails, you can restart the upload for that chunk only. This parallelism will improve your overall upload speed.

## Examples

Because bulk import is a complex mechanism to achieve performance reliability, you can have these short cuts to achieve your goal.

- [Bulk Import from CSV file](bulk-import-from-csv)
- [Bulk Import from TSV file](bulk-import-from-tsv)
- [Bulk Import from JSON file](bulk-import-from-json)
- [Bulk Import from Amazon S3](bulk-import-from-s3)
- [Bulk Import from MySQL](bulk-import-from-mysql)
- [Bulk Import from PostgreSQL](bulk-import-from-postgresql)
- [Bulk Import from MongoDB](bulk-import-from-mongodb)

To understand the bulk import internals, please refer the documents below.

- [Bulk Import Internals](bulk-import-internal)
