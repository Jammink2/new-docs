# Bulk Import

This article explains how to bulk-import data using the `td bulk_import` command.

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasure-data.com).

## Three Types of Data Import

#### 1) Streaming Import

Importing continuously generated data. In this case, the [td-agent](td-agent) article is more appropriate.

#### 2) Bulk Import

Importing huge amounts of data, which could cause a network transfer failure between your side and Treasure Data. This article covers this case.

#### 3) One-Time Import

If your data size is small (For example, the data fits in a single file), you can do an [One-Time-Import](one-time-import) instead. This is easier since your data does not have to be in MessagePack format.

## Why Bulk Import?

Because Treasure Data is a cloud service, the data needs to be transferred via an Internet network connection. This can get tricky once the data size gets big (> 100MB). Consider a couple of cases:

* If the network becomes unstable, the import could fail halfway through the data transfer. There's no easy way to pick up from where you left off, and you will need to restart the upload from scratch.
* Your company sometimes has bandwidth limits for transferring huge data with a single stream. Also, the limitations of the TCP/IP protocol make it difficult for applications to saturate a network connection.

We designed our bulk import feature to overcome these problems. You can now break your larger data sets into smaller chunks and upload them *in parallel*. If the upload of a particular chunk fails, you can restart the upload for that chunk only. This parallelism will improve your overall upload speed.

## Bulk Import Steps

### 1) Create a Session

First create a `session` (a single instance containing multiple parts). One session is associated with one table.

    :::term
    $ td bulk_import:create bulk_2012 mydb mytable

### 2) Convert Your Data into Parts Files

Let's say you have CSV files in the ./logs_201208/ directory. You will first need to convert these files into MessagePack.gz format (our internal format for efficient storage). To do this, please run the following commands:

NOTE: Splitting the data into separate directories by month is recommended. You can also split it by date if you have more than 1GB/day of data post-compression.

    :::term
    $ td bulk_import:prepare_parts ./logs_201208/*.csv \
        --format csv \
        --columns time,uid,price,count --time-column 'time' \
        -o ./parts_201208/

If your files include column names in the first line, please use the --column-header option.

    :::term
    $ td bulk_import:prepare_parts ./logs_201208/*.csv \
        --format csv \
        --column-header \
        --time-column 'time' \
        -o ./parts_201208/

We support csv, tsv, and json formats for the --format option. Here are the remaining examples.

    :::term
    # TSV
    $ td bulk_import:prepare_parts ./logs_201208/*.tsv \
        --format tsv \
        --column-header \
        --time-column 'time' \
        -o ./parts_201208/
    
    # JSON
    $ td bulk_import:prepare_parts ./logs_201208/*.json \
        --format json \
        --time-column 'time' \
        -o ./parts_201208/

### 2) Upload a Part to the Session

You can now upload the `parts` directory into the session.

    :::term
    $ td bulk_import:upload_parts bulk_2012 ./parts_201208/*

If the upload fails, you can retry any time. No data duplication occurs.

    $ td bulk_import:upload_parts bulk_2012 ./parts_201208/*

You can import multiple parts within a single session.

    $ td bulk_import:upload_parts bulk_2012 ./parts_201207/*
    $ td bulk_import:upload_parts bulk_2012 ./parts_201206/*
    $ td bulk_import:upload_parts bulk_2012 ./parts_201205/*
    ...

The uploaded parts can be viewed using `bulk_import:show`.

    :::term
    $ td bulk_import:show bulk_2012

### 3) Perform

The `bulk_import:perform` command converts the uploaded files into a more efficient format on the cloud (This is why it takes a bit of time!). The files are converted in parallel by executing MapReduce jobs.

    :::term
    $ td bulk_import:freeze bulk_2012
    $ td bulk_import:perform bulk_2012 -w
    $ td bulk_import:list
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+
    | Name | Table     | Status | Frozen | JobID | Valid Parts | Error Parts | Valid Records | Error Records |
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+
    | t01  | sfdb.bi02 | Ready  |        | 70220 | 2           | 1           | 100           | 10            |
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+

### 4) Check Error Records

After running the `perform` command, some records could become invalid. You can list these invalid records with the `bulk_import:error_records` command.

    :::term
    $ td bulk_import:error_records bulk_2012

### 5) Commit

Once you have confirmed that the uploaded data is correct, use the `bulk_import:commit` command to transactionally import the data into the table.

    :::term
    $ td bulk_import:commit bulk_2012
    $ td bulk_import:delete bulk_2012
