# Bulk Import

This article explains how to bulk-import data using the `td import` command for **version 0.10.84 and above**.

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasure-data.com).
  * Java runtime (6 and above).

## Why Bulk Import?

Because Treasure Data is a cloud service, the data needs to be transferred via an Internet network connection. This can get tricky once the data size gets big (> 100MB). Consider a couple of cases:

* If the network becomes unstable, the import could fail halfway through the data transfer. There's no easy way to pick up from where you left off, and you will need to restart the upload from scratch.
* Your company sometimes has bandwidth limits for transferring huge data with a single stream. Also, the limitations of the TCP/IP protocol make it difficult for applications to saturate a network connection.

We designed our bulk import feature to overcome these problems. You can now break your larger data sets into smaller chunks and upload them *in parallel*. If the upload of a particular chunk fails, you can restart the upload for that chunk only. This parallelism will improve your overall upload speed.

## Bulk Import Mechanism: Prepare, Upload, Perform and Commit

<img style="display:block" src="/images/bulk-import-mechanism.png"/>

Bulk Import consists of four key steps: prepare, upload, perform and commit.

* Prepare: first, your data is transformed into the highly space-efficient gzipped MessagePack format. This step does not require network connection.
* Upload: then, the prepared data is uploaded into Treasure Data's (row-based) bulk upload storage system. The upload is done over a secure Internet connection.
* Perform: the uploaded data is then transformed into our column-oriented data format using MapReduce.
* Commit: After the perform step, the data is now compatible with Plazma (Treasure Data's columnar, distributed storage system). So, the commit step copies the data over into Plazma.

## Importing Large Data Reliably with Bulk Import (in ONE step)

In the section below, we will show you how to use Bulk Import step by step. While it is important to understand each step, it is often unwieldy to perform all these steps one by one. Thus we provide several command line options to automate (and pipeline) the prepare-upload-perform-commit workflow. Specifically, you want to run `td import:upload` like this:

    :::term
    $ td import:upload --auto-create my_db.my_table --auto-perform --auto-commit --column-header -o prepared_parts data_*.csv

For example, if
    
* the original files are data_*.csv
* the prepared data is stored in the directory "prepared_parts"
* the data is uploaded in my_db.my_table

you can run

    :::term
    $ td import:upload --auto-create my_db.my_table --auto-perform --auto-commit --column-header -o prepared_parts data_*.csv

A couple of assumptions for the example above:

1. We assume that the data is in CSV format.
2. We assume that the first row of each input file contains column names (This is done with the `--column-header` option. You can also treat all rows as data and specify column names with `--columns c1,c2,c3...` as well.)

## Importing Large Data Reliably with Bulk Import (Step by Step)

### 1) Create a Session

First create a `session` (a single instance containing multiple parts). One session is associated with one table.

    :::term
    $ td import:create bulk_2012 mydb mytable

### 2) Convert Your Data into Parts Files

Let's say you have CSV files in the ./logs_201208/ directory. You will first need to convert these files into MessagePack.gz format (our internal format for efficient storage). To do this, please run the following commands:

NOTE: Splitting the data into separate directories by month is recommended. You can also split it by date if you have more than 1GB/day of data post-compression.

    :::term
    $ td import:prepare ./logs_201208/*.csv \
        --format csv \
        --columns time,uid,price,count --time-column 'time' \
        -o ./parts_201208/

If your files include column names in the first line, please use the --column-header option.

    :::term
    $ td import:prepare ./logs_201208/*.csv \
        --format csv \
        --column-header \
        --time-column 'time' \
        -o ./parts_201208/

We support csv, tsv, and json formats for the --format option. Here are the remaining examples.

    :::term
    # TSV
    $ td import:prepare ./logs_201208/*.tsv \
        --format tsv \
        --column-header \
        --time-column 'time' \
        -o ./parts_201208/
    
    # JSON
    $ td import:prepare ./logs_201208/*.json \
        --format json \
        --time-column 'time' \
        -o ./parts_201208/

### 3) Upload a Part to the Session

You can now upload the `parts` directory into the session.

    :::term
    $ td import:upload bulk_2012 ./parts_201208/*

If the upload fails, you can retry any time. No data duplication occurs.

    $ td import:upload bulk_2012 ./parts_201208/*

You can import multiple parts within a single session.

    $ td import:upload bulk_2012 ./parts_201207/*
    $ td import:upload bulk_2012 ./parts_201206/*
    $ td import:upload bulk_2012 ./parts_201205/*
    ...

The uploaded parts can be viewed using `import:show`.

    :::term
    $ td import:show bulk_2012

### 4) Perform

The `import:perform` command converts the uploaded files into a more efficient format on the cloud (This is why it takes a bit of time!). The files are converted in parallel by executing MapReduce jobs.

    :::term
    $ td import:freeze bulk_2012
    $ td import:perform bulk_2012 -w
    $ td import:list
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+
    | Name | Table     | Status | Frozen | JobID | Valid Parts | Error Parts | Valid Records | Error Records |
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+
    | t01  | sfdb.bi02 | Ready  |        | 70220 | 2           | 1           | 100           | 10            |
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+

### 5) Check Error Records

After running the `perform` command, some records could become invalid. You can list these invalid records with the `import:error_records` command.

    :::term
    $ td import:error_records bulk_2012

### 6) Commit

Once you have confirmed that the uploaded data is correct, use the `import:commit` command to transactionally import the data into the table.

    :::term
    $ td import:commit bulk_2012
    $ td import:delete bulk_2012

## Does Bulk Import support X?

The first place to go is `td help import`.

    :::term
    $ td help import
    Additional commands, type "td help COMMAND" for more details:

      import:list                                # List bulk import sessions
      import:show <name>                         # Show list of uploaded parts
      import:create <name> <db> <table>          # Create a new bulk import session to the the table
      import:prepare <files...>                  # Convert files into part file format
      import:upload <name> <files...>            # Upload or re-upload files into a bulk import session
      import:perform <name>                      # Start to validate and convert uploaded files
      import:error_records <name>                # Show records which did not pass validations
      import:commit <name>                       # Start to commit a performed bulk import session
      import:delete <name>                       # Delete a bulk import session
      import:freeze <name>                       # Reject succeeding uploadings to a bulk import session
      import:unfreeze <name>                     # Unfreeze a frozen bulk import session

For subcommands, you can do `td help import:<subcommand name>`, e.g., `td help import:upload`

    :::term
    $ td help import:upload
    java version "1.6.0_51"
    Java(TM) SE Runtime Environment (build 1.6.0_51-b11-457-11M4509)
    Java HotSpot(TM) 64-Bit Server VM (build 20.51-b01-457, mixed mode)
    command: false
    usage:
      $ td bulk_import:upload_parts <name> <files...>

    example:
      $ td bulk_import:upload_parts parts/* --parallel 4

    description:
      Upload or re-upload files into a bulk import session

    options:
        -C, --compress TYPE              compressed type [gzip, none]; default=auto detect
        -T, --time-format FORMAT         STRF_FORMAT; strftime(3) format of the time column
        --auto-commit                    commit bulk import job automatically
        --auto-create DATABASE.TABLE     create automatically bulk import session by specified
                                           database and table names
        --auto-delete                    delete bulk import session automatically
        --auto-perform                   perform bulk import job automatically
        --column-header                  first line includes column names
        --column-types TYPE,TYPE,...     column types [string, int, long]
        --columns NAME,NAME,...          column names (use --column-header instead if the first line
                                           has column names)
        --delimiter CHAR                 delimiter CHAR; default="," at csv, "\t" at tsv
        -e, --encoding TYPE              encoding type [utf-8]
        --error-records-handling MODE    error records handling mode [skip, abort]; default=skip
        --exclude-columns NAME,NAME,...
                                         exclude columns
        -f, --format FORMAT              source file format [csv, tsv, json, msgpack]; default=csv
        --newline TYPE                   newline [CRLR, LR, CR];  default=CRLF
        -o, --output DIR                 output directory
        --only-columns NAME,NAME,...     only columns
        --parallel NUM                   upload in parallel (default: 2; max 8)
        --prepare-parallel NUM           prepare in parallel (default: 2; max 8)
        --quote CHAR                     quote [DOUBLE, SINGLE]; default=DOUBLE
        -s, --split-size SIZE_IN_KB      size of each parts (default: 16384)
        -t, --time-column NAME           name of the time column
        --time-value TIME                long value of the time column
