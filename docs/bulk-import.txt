# Bulk Import

This guide explains how to bulk-import data with the `td bulk_import` command.

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasure-data.com).

## Three Categories of Import

#### 1) Streaming Import

The import for continuously generated data. In this case, [td-agent](td-agent) article is more appropriate.

#### 2) Bulk Import

The import for huge amounts of data, which could cause a network transfer failure between your side and Treasure Data. In this case, please proceed to the folowing sections.

#### 3) One-Time Import

The import for importing small amount of data, usually stored within one file, which rarely causes the transfer failrue. In this case, please refer [One-Time Import](one-time-import) article.

## About Bulk Import Feature

Because Treasure Data is the cloud services, the data needs to be transferred via an internet network connection. That causes the problems around huge data (> 100MB) uploads. Those are the examples:

* If the network becomes unstable, the import could fail in halfway through. There's no easy way to pick up from where you left off, and you need to restart the upload from the beggining.
* Your company sometimes limits the bandwidth for transferring the huge data with a single stream. Also, the limitations of the TCP/IP protocol make it very difficult for applications to saturate a network connection.

To overcome those problems, the bulk import feature was introduced. You can now break your larger data sets into smaller chunks, and upload them in parallel. If the upload of a chunk fails, you can simply restart it per chunk. You'll be able to impove your overall upload speed by taking advantage of parallelism.

## Bulk Import Steps

### 0) Split Your Data into MessagePack.gz Files

At first, you need to split your data into [MessagePack](http://msgpack.org/).gz format. Split into the monthly data is preferrable, but could be splitted into daily data if you have >1GB daily compressed data.

### 1) Create a Session

At first you need to create a `session`, which denotes the single instance containing the multiple parts. The session is associated with one table.

    :::bash
    $ td bulk_import:create bulk_201203 mydb mytable

### 2) Upload a Part to the Session

Then, you can upload the `parts` into the session. Those uploads can be done in parallel.

    :::bash
    $ td bulk_import:upload_part bulk_201203 01d 01d.msgpack.gz
    $ td bulk_import:upload_part bulk_201203 02d 02d.msgpack.gz
    $ td bulk_import:upload_part bulk_201203 03d 03d.msgpack.gz

If the upload failed in halfway through, you can retry any time.

    $ td bulk_import:upload_part bulk_201203 01d 01d.msgpack.gz

Now, let's list the uploaded parts by `bulk_import:show`.

    :::bash
    $ td bulk_import:show bulk_201203

### 3) Perform

`bulk_import:perform` does a format conversion within a cloud. Your data is converted into more efficient format, and it takes time. The conversion is done in parallel, by execuating MapReduce jobs.

    :::bash
    $ td bulk_import:freeze bulk_201203
    $ td bulk_import:perform bulk_201203 -w
    $ td bulk_import:list bulk_201203
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+
    | Name | Table     | Status | Frozen | JobID | Valid Parts | Error Parts | Valid Records | Error Records |
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+
    | t01  | sfdb.bi02 | Ready  |        | 70220 | 2           | 1           | 100           | 10            |
    +------+-----------+--------+--------+-------+-------------+-------------+---------------+---------------+

### 3) Check Error Records

After perform, some records could be invalid. You can list those invalid records by `bulk_import:error_records`.

    :::bash
    $ td bulk_import:error_records bulk_201203

### 4) Commit

If the uploaded data is correct, `bulk_import:commit` command will transactionally import the data into the table.

    :::bash
    $ td bulk_import:commit bulk_201203
    $ td bulk_import:delete bluk_201203
