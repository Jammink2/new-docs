# Scheduled Jobs (Web Console)

Treasure Data has a scheduler feature that supports periodic query execution. We take great care in distributing and operating our scheduler in order to achieve high availability. By using this feature, you no longer need a `cron` daemon on your local datacenter.

## Prerequisites

  * Basic knowledge of Treasure Data
  * A table with some data. An example is provided in the [Getting Started](quickstart) guide.

## Create the Schedule (Web Console)

A new schedule can be created on [Web Console](https://console.treasuredata.com/schedules). Please hit "New Query" button, and set the schedule for the query.

### Cron Schedule

There are two predefined Cron schedules:

Type | Description | Cron Equivalent
:--: | :---------: | :-------------:
@daily   | Run once a day at midnight | <tt>0 0 \* \* \*</tt>
@monthly | Run once a month at midnight on the morning of the first day of the month | <tt>0 0 1 \* \*</tt>

For any other scheduling definition, please select the 'Other cron...' option. The cron specification format is based off the [<tt>cron-spec</tt> gem](https://github.com/j0hnds/cron-spec). The five required fields are:

     *    *    *    *    *
     -    -    -    -    -
     |    |    |    |    |
     |    |    |    |    +----- day of week (0 - 6) (Sunday=0)
     |    |    |    +---------- month (1 - 12)
     |    |    +--------------- day of month (1 - 31)
     |    +-------------------- hour (0 - 23)
     +------------------------- min (0 - 59)

The following named entries can be used:

* Day of Week: sun, mon, tue, wed, thu, fri, sat
* Month: jan, feb, mar, apr, may, jun, jul, aug, sep, oct, nov, dec

A single space is required between each field. The values for each of the field can be composed of:

* a single value, within the limits displayed above for each field.
* a wilcard <tt>'\*'</tt> to indicate no restriction based on the field.
  E.g. <tt>'0 0 1 \* \*'</tt> configures the schedule to run at midnight (00:00) on the first day of each month.
* a range <tt>'2-5'</tt>, indicating the range of accepted values for the field.
  E.g. <tt>'0 0 1-10 \* \*'</tt> configures the schedule to run at midnight (00:00) on the first 10 days of each month.
* a list of comma-separated values <tt>'2,3,4,5'</tt>, indicating the list of accepted values for the field.
  E.g. <tt>'0 0 1,11,21 \* \*'</tt> configures the schedule to run at midnight (00:00) every 1st, 11th, and 21st day of each month.
* a periodicity indicator <tt>'\*/5'</tt> to express how often based on the field's valid range of values a schedule is allowed to run.
  E.g. <tt>'0 0 \*/5 \* \*'</tt> configure the schedule to run at midnight (00:00) every 5 days starting on the 5th of each month.
* a comma-separated list of any of the above except the <tt>'\*'</tt> widlcard is also supported <tt>'2,*/5,8-10'</tt>.
  E.g. <tt>'0 0 5,\*/10,25 \* \*'</tt> configures the schedule to run at midnight (00:00) every 5th, 10th, 20th, and 25th day of each month.

When two specification provide conflicting schedule specifications, the specification requesting to execute more often is followed while the other is ignored. For example, if the cron schedule is <tt>'0 0 0 \* 1'</tt>, the 'day of month' specification and 'day of week' are discordant because the former requires to run every 1st day of each month at midnight (00:00) while the latter requires to run every Monday at midnight (00:00): in this case the latter is followed. The same concept applies to all other fields.

For more information, please refer to this the [Wikipedia page on Cron](http://en.wikipedia.org/wiki/Cron#CRON_expression) and the Crontab Linux 'man' page](http://linux.die.net/man/5/crontab).

## Example: Daily KPI

A common pattern is to periodically calculate the fixed KPIs or metrics in a certain interval.

    :::sql
    SELECT
      TD_TIME_FORMAT(TIME, "yyyy-MM-dd") AS day,
      COUNT(1) AS cnt
    FROM
      www_access
    GROUP BY
      TD_TIME_FORMAT(TIME, "yyyy-MM-dd"),
    WHERE
      TD_TIME_RANGE(time,
        TD_TIME_ADD(TD_SCHEDULED_TIME(), '-1d'),
        TD_SCHEDULED_TIME())

The example above aggregates the daily page views from an access log on a daily basic. It makes use of several common [UDFs](/articles/udfs) to set the proper time range for the aggregation. TD_SCHEDULED_TIME() returns the time when the job gets scheduled to be run.

Please set the following parameter to create the scheduled jobs. Especially by setting the delay, you can wait for a certain amount of time to wait the data import. In this case, the job will be launched at 1AM (3600 seconds delay), but `TD_SCHEDULED_TIME()` returns 12am.

- Recurring?: @daily
- Delay (seconds): 3600

You can write the result into another system, to track the KPIs.

- [Job Result Output](/categories/result)

## Example: List of Daily Active Users

Another common pattern is to periodically calculate the another Treasure Data table, which is used by another jobs.

    :::sql
    SELECT
      user_id
    FROM
      www_access
    GROUP BY
      user_id
    WHERE
      TD_TIME_RANGE(time,
        TD_TIME_ADD(TD_SCHEDULED_TIME(), '-1d'),
        TD_SCHEDULED_TIME())

The example above aggregates the list of daily active users from an access log on a daily basic. Please set the following parameter to create the scheduled jobs.

- Recurring?: @daily
- Delay (seconds): 3600

You can [write this list to another Treasure Data table](result-into-td) by setting these parameters. Because writing the result into another table is atomic, you can use this table from other queries anytime.

- Export Result To: Treasure Data
- Database: YOUR_DATABASE
- Table: YOUR_TABLE
- Mode: Replace

## Example: Data Mart Generation

A common pattern is to periodically summarize data from logs, and create the datamart within RDBMS etc.

    :::sql
    SELECT
      user, code, method, path, agent, host, avg(size)
    FROM
      www_access
    GROUP BY
      user, code, method, path, agent, host
    WHERE
      TD_TIME_RANGE(time,
        TD_TIME_ADD(TD_SCHEDULED_TIME(), '-1h'),
        TD_SCHEDULED_TIME())

The example above aggregates web request results per user, code, path, agent, host, and average size from an access log on an hourly basis.

Please set the following parameter to create the scheduled jobs. Especially by setting the delay, you can wait for a certain amount of time to wait the data import.

- Recurring?: @hourly
- Delay (seconds): 600

You can write the result into another RDBMSs, to slice-and-dice the data from BI tools etc.

- [Job Result Output](/categories/result)
