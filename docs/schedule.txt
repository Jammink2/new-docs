# Scheduled Jobs (Web Console)

Treasure Data has a scheduler feature that supports periodic query execution. We take great care in distributing and operating our scheduler in order to achieve high availability. By using this feature, you no longer need a `cron` daemon on your local datacenter.

## Prerequisites

  * Basic knowledge of Treasure Data
  * A table with some data. An example is provided in the [Getting Started](quickstart) guide.

## Create the Schedule (Web Console)

A new schedule can be created on [Web Console](https://console.treasuredata.com/schedules). Please hit "New Query" button, and set the schedule for the query.

## Example: Daily KPI

A common pattern is to periodically calculate the fixed KPIs or metrics in a certain interval.

    :::sql
    SELECT
      TD_TIME_FORMAT(TIME, "yyyy-MM-dd") AS day,
      COUNT(1) AS cnt
    FROM
      www_access
    GROUP BY
      TD_TIME_FORMAT(TIME, "yyyy-MM-dd"),
    WHERE
      TD_TIME_RANGE(time,
        TD_TIME_ADD(TD_SCHEDULED_TIME(), '-1d'),
        TD_SCHEDULED_TIME())

The example above aggregates the daily page views from an access log on a daily basic. It makes use of several common [UDFs](/articles/udfs) to set the proper time range for the aggregation. TD_SCHEDULED_TIME() returns the time when the job gets scheduled to be run.

Please set the following parameter to create the scheduled jobs. Especially by setting the delay, you can wait for a certain amount of time to wait the data import. In this case, the job will be launched at 1AM (3600 seconds delay), but `TD_SCHEDULED_TIME()` returns 12am.

- Recurring?: @daily
- Delay (seconds): 3600

You can write the result into another system, to track the KPIs.

- [Job Result Output](/categories/result)

## Example: List of Daily Active Users

Another common pattern is to periodically calculate the another Treasure Data table, which is used by another jobs.

    :::sql
    SELECT
      user_id
    FROM
      www_access
    GROUP BY
      user_id
    WHERE
      TD_TIME_RANGE(time,
        TD_TIME_ADD(TD_SCHEDULED_TIME(), '-1d'),
        TD_SCHEDULED_TIME())

The example above aggregates the list of daily active users from an access log on a daily basic. Please set the following parameter to create the scheduled jobs.

- Recurring?: @daily
- Delay (seconds): 3600

You can [write this list to another Treasure Data table](result-into-td) by setting these parameters. Because writing the result into another table is atomic, you can use this table from other queries anytime.

- Export Result To: Treasure Data
- Database: YOUR_DATABASE
- Table: YOUR_TABLE
- Mode: Replace

## Example: Data Mart Generation

A common pattern is to periodically summarize data from logs, and create the datamart within RDBMS etc.

    :::sql
    SELECT
      user, code, method, path, agent, host, avg(size)
    FROM
      www_access
    GROUP BY
      user, code, method, path, agent, host
    WHERE
      TD_TIME_RANGE(time,
        TD_TIME_ADD(TD_SCHEDULED_TIME(), '-1h'),
        TD_SCHEDULED_TIME())

The example above aggregates web request results per user, code, path, agent, host, and average size from an access log on an hourly basis.

Please set the following parameter to create the scheduled jobs. Especially by setting the delay, you can wait for a certain amount of time to wait the data import.

- Recurring?: @hourly
- Delay (seconds): 600

You can write the result into another RDBMSs, to slice-and-dice the data from BI tools etc.

- [Job Result Output](/categories/result)
