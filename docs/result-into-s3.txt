# Writing Query Results to S3

This article explains how to write job results directly to S3.

## Prerequisites

  * Basic knowledge of Treasure Data, including [the toolbelt](http://toolbelt.treasure-data.com).
  * A S3 bucket with write permission.

## Basic Usage

### For On-demand Jobs

For on-demand jobs, just add the `--result` option to the `td query` command. After the job is finished, the results are written to the S3 bucket with the given name and at the given path.

    :::term
    $ td query \
      --result 's3://acesskey:secretkey@/bucketname/path.csv' \
      -w -d testdb \
      "SELECT v['code'] AS code, COUNT(1) AS cnt FROM www_access GROUP BY v['code']"

NOTE: The access key and secret key must be url encoded.

For security reasons, you may want to use [AWS IAM](http://aws.amazon.com/iam/) to manage permission with respect to writing to S3.

The result file is in CSV format ([RFC 4180](http://www.ietf.org/rfc/rfc4180.txt)) where the first line is a header with the column names, new line is CRLF, text encoding is UTF-8, and quotation character is double quotes.

    :::csv
    _c0,_c1
    a,b
    c," d "
    e,f

### For Scheduled Jobs

For scheduled jobs, just add the `--result` option when scheduling a job. After every job run, the results are sent in the same manner as on-demand jobs.

    :::term
    $ td result:create mys3 ftp://user:password@domain.com/bucketname/
    $ td sched:create hourly_count_example "0 * * * *" \
      -d testdb \
      --result mys3:path.csv \
      "SELECT v['code'] AS code, COUNT(1) AS cnt FROM www_access GROUP BY v['code']"

